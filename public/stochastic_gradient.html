<!doctype html>
<meta charset="utf-8">
<meta name="viewport" content="width=1080">
<script src="assets/lib/template.v1.js"></script>
<script type="text/front-matter">
  title: Stochastic Gradient
  authors:
    - Fabian Pedregosa: http://fa.bianp.net
  affiliations:
    - UC Berkeley: http://bair.berkeley.edu/
    - ETH Zurich: http://www.inf.ethz.ch/
</script>

<!-- OpenGraph Info -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
        Macros: {
          RR: "{\\mathbb{R}}",
          argmin: "{\\operatorname*{\\mathrm{arg\\,min}}}",
          minimize: "{\\operatorname*{\\mathrm{minimize}}}",
          bold: ["{\\bf #1}",1],
          xx: "{\\boldsymbol x}",
          uu: "{\\boldsymbol u}",
          yy: "{\\boldsymbol y}",
          ss: "{\\boldsymbol s}",
        }
      },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<link rel="stylesheet" type="text/css" href="assets/widgets.css">

<!-- Required -->
<script src="assets/lib/lib.js"></script>
<script src="assets/utils.js"></script>
<script>
  var renderQueue = [];
  function renderMath(elem) {
    // renderMathInElement(
    //     elem,
    //     {
    //         delimiters: [
    //             {left: "$$", right: "$$", display: true},
    //             {left: "$", right: "$", display: false},
    //         ]
    //     }
    // );
  }

  var deleteQueue = [];
  function renderLoading(figure) {
    var loadingScreen = figure.append("svg")
    .style("width", figure.style("width"))
    .style("height", figure.style("height"))
    .style("position","absolute")
    .style("top", "0px")
    .style("left","0px")
    .style("background","white")
    .style("border", "0px dashed #DDD")
    .style("opacity", 1)

    return function(callback) { loadingScreen.remove() };

  }

</script>
<div id="math-cache" style="display: none;">
  <dt-math class="star">\star</dt-math>
  <dt-math class="plus">+</dt-math>
  <dt-math class="minus">-</dt-math>
  <dt-math class="equals">=</dt-math>
  <dt-math class="alpha">\alpha</dt-math>
  <dt-math class="lambda">\lambda</dt-math>
  <dt-math class="beta">\beta</dt-math>
  <dt-math class="r">R</dt-math>
  <dt-math class="alpha-equals">\alpha=</dt-math>
  <dt-math class="beta-equals">\beta=</dt-math>
  <dt-math class="beta-equals-zero">\beta = 0</dt-math>
  <dt-math class="beta-equals-one">\beta=1</dt-math>
  <dt-math class="alpha-equals-one-over-lambda-i">\alpha = 1/\lambda_i</dt-math>
  <dt-math class="model">\text{model}</dt-math>
  <dt-math class="p">0 p_1</dt-math>
  <dt-math class="phat">0 \bar{p}_1</dt-math>
  <dt-math class="two-sqrt-beta">2\sqrt{\beta}</dt-math>
  <dt-math class="lambda-i">\lambda_i</dt-math>
  <dt-math class="lambda-i-equals-zero">\lambda_i = 0</dt-math>
  <dt-math class="alpha-gt-one-over-lambda-i">\alpha > 1/\lambda_i</dt-math>
  <dt-math class="max-sigma-one">\max\{|\sigma_1|,|\sigma_2|\} > 1</dt-math>
  <dt-math class="x-i-k">x_i^k - x_i^*</dt-math>
  <dt-math class="xi-i">\xi_i</dt-math>
  <dt-math class="beta-equals-one-minus">\beta = (1 - \sqrt{\alpha \lambda_i})^2</dt-math>
</div>
<script>
  function MathCache(id) {
    return document.querySelector("#math-cache ." + id).innerHTML;
  }
</script>
<svg style="display: none;">
  <g id="pointerThingy">
    <circle fill="none" stroke="#FF6C00" stroke-linecap="round" cx="0" cy="0" r="14"/>
    <circle fill="#FF6C00" cx="0" cy="0" r="11"/>
    <path id="XMLID_173_" fill="#FFFFFF" d="M-3.2-1.3c0-0.1,0-0.2,0-0.3c0-0.1,0-0.2,0-0.3c-0.6,0-1.2,0-1.8,0c0,0.6,0,1.2,0,1.8
      c0.2,0,0.4,0,0.6,0c0-0.4,0-0.8,0-1.2c0,0,0.1,0,0.1,0c0.3,0,0.5,0,0.8,0C-3.4-1.3-3.3-1.3-3.2-1.3c0,0.2,0,0.4,0,0.6
      c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0,0,0,0-0.1c0-1.6,0-3.2,0-4.8c0-0.6,0-1.2,0-1.8c0,0,0,0,0.1,0
      c0.3,0,0.7,0,1,0c0.1,0,0.1,0,0.2,0c0-0.2,0-0.4,0-0.6c-0.4,0-0.8,0-1.2,0C-2-7.2-2-7-2-6.8c0,0,0,0-0.1,0c-0.2,0-0.3,0-0.5,0
      c0,0,0,0-0.1,0c0,1.8,0,3.6,0,5.5c-0.2,0-0.3,0-0.4,0C-3.1-1.3-3.2-1.3-3.2-1.3z M1.1-3.7C1-3.8,1-3.8,1.1-3.7C1-4,1-4.1,1-4.3
      c0,0,0,0,0-0.1c-0.4,0-0.8,0-1.2,0c0-0.8,0-1.6,0-2.4c-0.2,0-0.4,0-0.6,0c0,1.8,0,3.6,0,5.5c0.2,0,0.4,0,0.6,0c0-0.8,0-1.6,0-2.4
      c0,0,0.1,0,0.1,0C0.3-3.7,0.6-3.7,1.1-3.7C1-3.7,1-3.7,1.1-3.7C1.1-3.7,1-3.7,1.1-3.7c0,0.8,0,1.6,0,2.3c0,0,0,0.1,0,0.1
      c0.2,0,0.4,0,0.6,0c0-0.6,0-1.2,0-1.8c0.4,0,0.8,0,1.2,0c0,0.8,0,1.6,0,2.4c0.2,0,0.4,0,0.6,0c0-0.6,0-1.2,0-1.8c0.2,0,0.4,0,0.6,0
      c0,0,0,0,0,0.1c0,0.1,0,0.3,0,0.4c0,0,0,0.1,0,0.1c0.2,0,0.4,0,0.5,0c0,0,0.1,0,0.1,0.1c0,0.2,0,0.5,0,0.7c0,1.1,0,2.3,0,3.4
      c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0c0,0,0,0,0,0c0,0.6,0,1.1,0,1.7c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0c0,0.4,0,0.8,0,1.2
      c-1.6,0-3.2,0-4.9,0c0-0.4,0-0.8,0-1.2c-0.2,0-0.4,0-0.6,0C-2,3.8-2,3.4-2,3c-0.2,0-0.4,0-0.6,0c0,0.4,0,0.8,0,1.2
      c0.2,0,0.4,0,0.6,0C-2,4.8-2,5.4-2,6c2,0,4.1,0,6.1,0c0-0.1,0-0.2,0-0.3c0-0.5,0-0.9,0-1.4c0-0.1,0-0.1,0-0.2c0.2,0,0.4,0,0.5,0
      c0.1,0,0.1,0,0.1-0.1c0-0.4,0-0.9,0-1.3c0-0.1,0-0.3,0-0.4c0.1,0,0.2,0,0.3,0c0.1,0,0.2,0,0.3,0c0-1.4,0-2.8,0-4.3
      c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c-0.4,0-0.8,0-1.2,0c0-0.2,0-0.4,0-0.6
      c-0.1,0-0.2,0-0.3,0c-0.4,0-0.9,0-1.3,0C1.2-3.7,1.1-3.7,1.1-3.7z M-3.2,1.8c0,0.4,0,0.8,0,1.2c0.2,0,0.4,0,0.5,0
      c0.1,0,0.1,0,0.1-0.1c0-0.3,0-0.6,0-1c0-0.1,0-0.1,0-0.2C-2.8,1.8-3,1.8-3.2,1.8c0-0.4,0-0.8,0-1.2c-0.2,0-0.4,0-0.6,0
      c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0,0,0,0,0.1c0,0.1,0,0.3,0,0.4c0,0.2,0,0.5,0,0.7
      c0,0,0,0.1,0.1,0.1c0.1,0,0.2,0,0.3,0C-3.4,1.8-3.3,1.8-3.2,1.8z"/>
    <path id="XMLID_172_" fill="#FFFFFF" d="M4.1,4.2C4.1,4.2,4.1,4.2,4.1,4.2c0-0.6,0-1.2,0-1.8c0,0,0,0,0,0c0.2,0,0.4,0,0.6,0
      c0,0,0-0.1,0-0.1c0-1.1,0-2.3,0-3.4c0-0.2,0-0.5,0-0.7c0,0,0-0.1-0.1-0.1c-0.2,0-0.4,0-0.5,0c0,0,0-0.1,0-0.1c0-0.1,0-0.3,0-0.4
      c0,0,0-0.1,0-0.1c-0.2,0-0.4,0-0.6,0c0,0.6,0,1.2,0,1.8c-0.2,0-0.4,0-0.6,0c0-0.8,0-1.6,0-2.4c-0.4,0-0.8,0-1.2,0
      c0,0.6,0,1.2,0,1.8c-0.2,0-0.4,0-0.6,0c0,0,0-0.1,0-0.1c0-0.7,0-1.5,0-2.2c0,0,0-0.1,0-0.1l0,0c0.1,0,0.2,0,0.2,0
      c0.4,0,0.9,0,1.3,0c0.1,0,0.2,0,0.3,0c0,0.2,0,0.4,0,0.6c0.4,0,0.8,0,1.2,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6
      c0.2,0,0.4,0,0.6,0c0,1.4,0,2.8,0,4.3c-0.1,0-0.2,0-0.3,0c-0.1,0-0.2,0-0.3,0c0,0.1,0,0.3,0,0.4c0,0.4,0,0.9,0,1.3
      c0,0.1,0,0.1-0.1,0.1C4.5,4.2,4.3,4.2,4.1,4.2L4.1,4.2z"/>
    <path id="XMLID_171_" fill="#FFFFFF" d="M4.1,4.2c0,0.1,0,0.1,0,0.2c0,0.5,0,0.9,0,1.4c0,0.1,0,0.2,0,0.3C2.1,6,0,6-2,6
      c0-0.6,0-1.2,0-1.8c-0.2,0-0.4,0-0.6,0c0-0.4,0-0.8,0-1.2C-2.4,3-2.2,3-2,3c0,0.4,0,0.8,0,1.2c0.2,0,0.4,0,0.6,0c0,0.4,0,0.8,0,1.2
      c1.6,0,3.2,0,4.9,0c0-0.4,0-0.8,0-1.2C3.7,4.2,3.9,4.2,4.1,4.2L4.1,4.2z"/>
    <path id="XMLID_170_" fill="#FFFFFF" d="M-2-6.8c0,0.6,0,1.2,0,1.8c0,1.6,0,3.2,0,4.8c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0
      c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6l0,0c0.1,0,0.1,0,0.2,0c0.1,0,0.3,0,0.4,0c0-1.8,0-3.6,0-5.5
      c0,0,0.1,0,0.1,0C-2.4-6.8-2.2-6.8-2-6.8C-2.1-6.8-2-6.8-2-6.8L-2-6.8z"/>
    <path id="XMLID_169_" fill="#FFFFFF" d="M1.1-3.7C1-3.7,1-3.7,1.1-3.7c-0.4,0-0.8,0-1.2,0c0,0,0,0-0.1,0c0,0.8,0,1.6,0,2.4
      c-0.2,0-0.4,0-0.6,0c0-1.8,0-3.6,0-5.5c0.2,0,0.4,0,0.6,0c0,0.8,0,1.6,0,2.4c0.4,0,0.8,0,1.2,0c0,0,0,0.1,0,0.1C1-4.1,1-4,1.1-3.7
      C1-3.8,1-3.8,1.1-3.7L1.1-3.7z"/>
    <path id="XMLID_168_" fill="#FFFFFF" d="M-3.2,1.8c-0.1,0-0.2,0-0.3,0c-0.1,0-0.2,0-0.3,0c0,0-0.1,0-0.1-0.1c0-0.2,0-0.5,0-0.7
      c0-0.1,0-0.3,0-0.4c0,0,0,0,0-0.1c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0
      C-3.2,0.9-3.2,1.3-3.2,1.8c0.2,0,0.4,0,0.6,0c0,0.1,0,0.1,0,0.2c0,0.3,0,0.6,0,1C-2.6,3-2.7,3-2.7,3c-0.2,0-0.3,0-0.5,0
      C-3.2,2.6-3.2,2.2-3.2,1.8z"/>
    <path id="XMLID_167_" fill="#FFFFFF" d="M-3.2-1.3c-0.1,0-0.2,0-0.3,0c-0.3,0-0.5,0-0.8,0c0,0,0,0-0.1,0c0,0.4,0,0.8,0,1.2
      c-0.2,0-0.4,0-0.6,0c0-0.6,0-1.2,0-1.8c0.6,0,1.2,0,1.8,0c0,0.1,0,0.2,0,0.3C-3.2-1.5-3.2-1.4-3.2-1.3L-3.2-1.3z"/>
    <path id="XMLID_166_" fill="#FFFFFF" d="M-2-6.8C-2-7-2-7.2-2-7.4c0.4,0,0.8,0,1.2,0c0,0.2,0,0.4,0,0.6c-0.1,0-0.1,0-0.2,0
      C-1.3-6.8-1.6-6.8-2-6.8C-2-6.8-2-6.8-2-6.8L-2-6.8z"/>
  </g>
</svg>

<dt-article class="centered">
  <h1>The Stochastic Gradient Method</h1>
  <dt-byline class="l-page"></dt-byline>

  <p>The Stochastic Gradient Method (SGM)<dt-cite key="robbins1951stochastic"></dt-cite> is one of the most widely-used methods for large-scale optimization and is one of the main methods behind the current AI revolution.</p>
  <figure>
    <img style="margin-left:100px;  width: 200px" src="https://upload.wikimedia.org/wikipedia/en/7/76/1966-HerbertRobbins.jpg" alt="">
   <figcaption id="expander" style="position:absolute; left:370px; top:10px; width:200px">Herbert Robbins was an American mathematician and statistician. Together with Sutton Monro, invented the stochastic gradient method, also known as the Robbinsâ€“Monro method.</figcaption>
  </figure>

  <p>
    Stochastic Gradient Method (also known as stochastic gradient descent or SGD) can be used to solve optimization problems in which the objective function is of the form $f(\xx) = \mathbb{E}[f_i(\xx)]$, where the expectation is taken with respect to $i$. The most comnmon case is when $i$ can take a finite number of values, in which the problem becomes
  </p>
  <p style="background-color: #D2E4FC; padding: 1px; border-radius: 8px;">\begin{equation}\label{eq:fw_objective}
    \minimize_{\boldsymbol{x} \in \RR^p} \frac{1}{n} \sum_{i=1}^n f_i(\boldsymbol{x}) ~,
  \end{equation}</p>
  <p>This class of problems arises often in machine learning, where the $f_i$ is the cost of mis-classifying each element in the dataset.
  </p>

  <p>The Stochastic Gradient Method can motivated as an approximation to gradient descent in which at each iteration we approximate the gradient as $\nabla f_i(\xx_t) \approx \nabla f(\xx_t)$:</p>

  <h2>The Stochastic Gradient Method</h2>
  <p>We can write the full gradient descent algorithm as follows. The algorithm only has one free parameter: the step size $\gamma$.</p>
  <p class="framed">\begin{align}
       &amp;\textbf{Input}: \text{initial guess $\xx_0$, step size sequence $\gamma_t > 0$}\nonumber\\
       &amp; \textbf{For }t=0, 1, \ldots \textbf{ do } \\
       &amp;\quad\text{Choose $i \in \{1, 2, \ldots, n \}$ uniformly at random} \\
      &amp;\quad\boldsymbol{x}_{t+1} = \boldsymbol{x}_t - \gamma_t \nabla f_i(\xx_t)~.\label{eq:update_rule}\\
      &amp;\textbf{end For loop}\\
      &amp; \textbf{return } \xx_t
      \end{align}
      </p>
      <p>SGM can be much more efficient than gradient descent in the case in which the objective consists of a large sum because at each iteration we only need to evaluate a partial gradient and not the full gradient. </p>
  <h2>Step size</h2>
  <p>
    The choice of step size is one of the most delicate aspects of SGM. In this case, backtracking line search is not an option since it would involve to evaluate the objective function at each iteration, destroying the computational advantage of this method.
  </p>
  <p>Two popular step size strategies exists for SGM, constant step size and decreasing step size
  <ul>
    <li>In the constant step size strategy, $\gamma_t=\gamma$ for some pre-determined constant $\gamma$. The method converges very fast to neighborbood of a local minima and the bounces around. The radius of this neighborhood will depend on the step size<dt-cite key="leblond2018improved,schmidt2014convergence,mandt2017stochastic"></dt-cite>.</li>
    <li>One can guarantee convergence to a local minimizer choosing a step size sequence that verifies
    $$
    \sum_{i=1}^\infty \gamma_t = \infty \text{ and } \sum_{i=1}^\infty \gamma_t^2 < \infty
    $$
    The most popular sequence to verify this is $\gamma_t = \frac{C}{t}$ for some constant $C$.
    This is often referred to as a "decreasing step-size" sequence, although in fact the sequence does not need to be monotonically decreasing.<dt-cite key="bottou2016optimization"></dt-cite>
    </li>
  </ul>
  </p>

<h2>Examples</h2>
<p>A least squares problem can be written in the form acceptable by SGD since
\begin{equation}
\frac{1}{n}\|\boldsymbol{A} \xx - \boldsymbol{b}\|^2 = \frac{1}{n} \sum_{i=1}^n (\boldsymbol{A}_i^T \xx - \boldsymbol{b}_i)^2
\end{equation}
where $\boldsymbol{A}_i$ is the $i$-th row of the matrix $\boldsymbol{A}$ .
</p>
<p>Stochastic Gradient with constant step-size</p>
  <figure style = "position:relative; width:984px; height:400px;">
    <div id="banana" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
    <div id="sliderAlpha" style="position:absolute; width:300px; height: 50px; left:20px; top: 320px;">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Step-size Î± = 0.2</text>
    </div>
    <figcaption id="Bananacaption" style="position:absolute; width: 420px; height: 90px; left: 540px; top: 320px;">
      Stochastic Gradient with constant step size converges quickly to a neighborhood of the optimum, but then bounces around.
    </figcaption>
  </figure>

  <dt-byline class="l-page"></dt-byline>
  <figure style = "position:relative; width:984px; height:400px;">
    <div id="banana2" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
    <div id="sliderAlpha2" style="position:absolute; width:300px; height: 50px; left:20px; top: 320px;">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Step-size Î± = 0.2</text>
    </div>

  </figure>


</figure>

<dt-byline class="l-page"></dt-byline>
<figure style = "position:relative; width:984px; height:400px;">
  <div id="banana3" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
  <div id="sliderAlpha3" style="position:absolute; width:300px; height: 50px; left:20px; top: 320px;">
    <text class="figtext" style="top: -5px; left: 20px; position: relative;">Step-size Î± = 0.05</text>
  </div>
  <figcaption id="Bananacaption" style="position:absolute; width: 420px; height: 90px; left: 540px; top: 320px;">
    Stochastic Gradient with decreasing step sizes is quite robust to the choice of step size. On one hand there is really no good way to set the step size (e.g., no equivalent of line search for Gradient Descent) but on the other hand it converges for a wide range of step sizes.
  </figcaption>

</figure>

<h2>SGD with momentum</h2>

<p>SGD is often augmented with a <i>momentum</i> term, so that the update in \eqref{eq:update_rule} becomes
$$
\boldsymbol{x}_{t+1} = \boldsymbol{x}_t - \gamma_t \nabla f_i(\xx_t) - \underbrace{\beta (\xx_t - \xx_{t-1})}_{\text{momentum}}
$$
</p>
<p>Momentum adds the previous update direction to the gradient. This has two beneficial effects:
<ul>
  <li>Stabilizing the update in case of outlayer gradients.</li>
  <li>Escaping regions of flat curvature.</li>
</ul>
</p>


<figure style = "position:relative; width:984px; height:400px;">
  <div id="banana5" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
  <div id="sliderAlpha5" style="position:absolute; width:300px; height: 50px; left:20px; top: 320px;">
    <text class="figtext" style="top: -5px; left: 20px; position: relative;">Step-size Î± = 0.2</text>
  </div>
  <figcaption id="Bananacaption" style="position:absolute; width: 420px; height: 90px; left: 540px; top: 320px;">
    SGD with a momentum coefficient of $\beta=0.2$
  </figcaption>
</figure>


<figure style = "position:relative; width:984px; height:400px;">
  <div id="banana4" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
  <div id="sliderAlpha4" style="position:absolute; width:300px; height: 50px; left:20px; top: 320px;">
    <text class="figtext" style="top: -5px; left: 20px; position: relative;">Step-size Î± = 0.05</text>
  </div>
  <figcaption id="Bananacaption" style="position:absolute; width: 420px; height: 90px; left: 540px; top: 320px;">
    SGD with a momentum coefficient of $\beta=0.2$
  </figcaption>
</figure>

<!-- <dt-byline class="l-page"></dt-byline>
<figure style = "position:relative; width:984px; height:400px;">
  <div id="banana5" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
  <div id="sliderAlpha5" style="position:absolute; width:300px; height: 50px; left:20px; top: 320px;">
    <text class="figtext" style="top: -5px; left: 20px; position: relative;">Step-size Î± = 0.2</text>
  </div>

</figure>


</figure>

<dt-byline class="l-page"></dt-byline>
<figure style = "position:relative; width:984px; height:400px;">
<div id="banana6" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
<div id="sliderAlpha6" style="position:absolute; width:300px; height: 50px; left:20px; top: 320px;">
  <text class="figtext" style="top: -5px; left: 20px; position: relative;">Step-size Î± = 0.02</text>
</div>
<figcaption id="Bananacaption" style="position:absolute; width: 420px; height: 90px; left: 540px; top: 320px;">
  Stochastic Gradient with decreasing step sizes is quite robust to the choice of step size. On one hand there is really no good way to set the step size (e.g., no equivalent of line search for Gradient Descent) but on the other hand it converges for a wide range of step sizes.
</figcaption>

</figure> -->



  <script src="assets/lib/contour_plot.js"></script>
  <script src="assets/iterates.js"></script>
  <script>

  // Render Foreground
  var iterControl = genIterDiagram(well_conditioned_quad, [0,0], [[-2,2],[2/3 + 0.4,-2/3 + 0.4]], runStochasticGradient, true, true, 300)
                    .alpha(0.003)
                    .beta(0)
                    (d3.select("#banana").style("position","relative"))

  var iterChange = iterControl.control
  var getw0 = iterControl.w0

  var StepRange = d3.scaleLinear().domain([0,100]).range([0,1.00])
  var MomentumRange = d3.scaleLinear().domain([0,100]).range([0,0.98])

  var update = function (i,j) { iterChange(i, 0, getw0()) }

  var slidera = sliderGen([230, 40])
              .ticks([0,0.1, 0.2])
              .ticktitles( function(d,i) { return ["0", "0.1", "0.2"][i]})
              .change( function (i) {
                d3.select("#sliderAlpha").selectAll(".figtext").html("Step-size Î± = " + getalpha().toPrecision(2) )
                iterChange(getalpha(), getbeta(), getw0() )
              } )
              .startxval(0.05)
              .cRadius(7)
              .shifty(-12)
              .margins(20,20)
  var getalpha = slidera( d3.select("#sliderAlpha")).xval
  var getbeta = function() {return 0;}
  // var getbeta  = sliderb( d3.select("#sliderBeta")).xval

  iterChange(getalpha(), getbeta(), getw0() )

  // Render Foreground
  var iterControl2 = genIterDiagram(well_conditioned_quad, [0,0], [[-2,2],[2/3 + 0.4,-2/3 + 0.4]], runStochasticGradientDecreasingSteps, true, true, 300)
                    .alpha(0.003)
                    .beta(0)
                    (d3.select("#banana2").style("position","relative"))

  var iterChange2 = iterControl2.control
  var getw02 = iterControl2.w0

  var update2 = function (i,j) { iterChange2(i, 0, getw02()) }

  var slidera2 = sliderGen([230, 40])
              .ticks([0, 0.5, 1.0])
              .ticktitles( function(d,i) { return ["0", "0.5", "1.0"][i]})
              .change( function (i) {
                d3.select("#sliderAlpha2").selectAll(".figtext").html("Step-size Î± = " + getalpha().toPrecision(2) )
                iterChange2(getalpha2(), getbeta2(), getw02() )
              } )
              .startxval(0.2)
              .cRadius(7)
              .shifty(-12)
              .margins(20,20)

  var getalpha2 = slidera2( d3.select("#sliderAlpha2")).xval
  var getbeta2 = function() {return 0;}
  // var getbeta  = sliderb( d3.select("#sliderBeta")).xval

  iterChange2(getalpha2(), getbeta2(), getw02() )


  // Render Foreground
  var iterControl3 = genIterDiagram(eyef, [0,0], [[-2,2],[2/3 + 0.4,-2/3 + 0.4]], runStochasticGradient, true, true, 300)
                    .alpha(0.003)
                    .beta(0)
                    (d3.select("#banana3").style("position","relative"))

  var iterChange3 = iterControl3.control
  var getw03 = iterControl3.w0

  var update3 = function (i,j) { iterChange3(i, 0, getw03()) }

  var slidera3 = sliderGen([230, 40])
              .ticks([0,0.01,0.02])
              .ticktitles( function(d,i) { return ["0", "0.01", "0.02"][i]})
              .change( function (i) {
                d3.select("#sliderAlpha3").selectAll(".figtext").html("Step-size Î± = " + getalpha().toPrecision(2) )
                iterChange3(getalpha3(), getbeta3(), getw03() )
              } )
              .startxval(0.003)
              .cRadius(7)
              .shifty(-12)
              .margins(20,20)

  var getalpha3 = slidera3( d3.select("#sliderAlpha3")).xval
  var getbeta3 = function() {return 0;}
  // var getbeta  = sliderb( d3.select("#sliderBeta")).xval

  iterChange3(getalpha3(), getbeta3(), getw03() )


  // ---

  // Render Foreground
  var iterControl4 = genIterDiagram(eyef, [0,0], [[-2,2],[2/3 + 0.4,-2/3 + 0.4]], runStochasticGradient, true, true, 300)
                    .alpha(0.003)
                    .beta(0.2)
                    (d3.select("#banana4").style("position","relative"))

  var iterChange4 = iterControl4.control
  var getw04 = iterControl4.w0

  var update4 = function (i,j) { iterChange4(i, 0, getw04()) }

  var slidera4 = sliderGen([230, 40])
              .ticks([0,0.01,0.02])
              .ticktitles( function(d,i) { return ["0", "0.01", "0.02"][i]})
              .change( function (i) {
                d3.select("#sliderAlpha4").selectAll(".figtext").html("Step-size Î± = " + getalpha().toPrecision(2) )
                iterChange4(getalpha4(), getbeta4(), getw04() )
              } )
              .startxval(0.003)
              .cRadius(7)
              .shifty(-12)
              .margins(20,20)

  var getalpha4 = slidera4( d3.select("#sliderAlpha4")).xval
  var getbeta4 = function() {return 0.2;}
  // var getbeta  = sliderb( d3.select("#sliderBeta")).xval

  iterChange4(getalpha4(), getbeta4(), getw04() )



  // Render Foreground
  var iterControl5 = genIterDiagram(well_conditioned_quad, [0,0], [[-2,2],[2/3 + 0.4,-2/3 + 0.4]], runStochasticGradientDecreasingSteps, true, true, 300)
                    .alpha(0.003)
                    .beta(0.2)
                    (d3.select("#banana5").style("position","relative"))

  var iterChange5 = iterControl5.control
  var getw05 = iterControl5.w0

  var update5 = function (i,j) { iterChange5(i, 0, getw05()) }

  var slidera5 = sliderGen([230, 40])
              .ticks([0, 0.5, 1.0])
              .ticktitles( function(d,i) { return ["0", "0.5", "1.0"][i]})
              .change( function (i) {
                d3.select("#sliderAlpha5").selectAll(".figtext").html("Step-size Î± = " + getalpha().toPrecision(2) )
                iterChange5(getalpha5(), getbeta5(), getw05() )
              } )
              .startxval(0.2)
              .cRadius(7)
              .shifty(-12)
              .margins(20,20)

  var getalpha5 = slidera5( d3.select("#sliderAlpha5")).xval
  var getbeta5 = function() {return 0.2;}
  // var getbeta  = sliderb( d3.select("#sliderBeta")).xval

  iterChange5(getalpha5(), getbeta5(), getw05() )


  </script>


<!-- <h2>Convergence</h2>
<p>Why does the SGM method converge despite its update being a very rough estimate of the gradient?. To answer this, we must first understand the <i>unbiasedness</i> property of its update.</p>
<p> <b>Unbiasedness of the SGM update</b>. Let $\mathbb{E}_t$ denote the expectation with respect to the choice of random sample ($i$) at iteration $t$. Then since the index $i$ is chosen <i>uniformly</i> at random we have
$$
\mathbb{E}_t \nabla f_{i_t}(\xx_t) = \sum_{i=1}^n \nabla f_{i}(\xx_t) P(i_t = i)
 = \frac{1}{n}\sum_{i=1}^n \nabla f_{i}(\xx_t) = \nabla f(\xx_t)$$
This is the crucial property that makes SGD work. For a full proof, see e.g.
 </p> -->


<!-- <p>
We remind the gradient-Lipschitz inequality, which states
\begin{align}
f(\yy) \leq f(\xx) + \langle \nabla f(\xx), \yy - \xx\rangle + \frac{L}{2}\|\xx - \yy\|^2
\end{align}
Using this inequality with $\yy=\xx_{t+1}$, $\xx = \xx_t$ we have
\begin{align}
f(\xx_{t+1}) &amp;\leq f(\xx_t) + \langle \nabla f(\xx_t), \xx_{t+1} - \xx_t\rangle + \frac{L}{2}\|\xx_t - \xx_{t+1}\|^2\\
&amp;=f(\xx_t) - \gamma_t \langle \nabla f(\xx_t), \nabla f_i(\xx_t)\rangle + \frac{\gamma_t^2 L}{2}\|\nabla f_i(\xx_t)\|^2\\
\end{align}
Taking expectations, and by the unbiasedness property outlined before we get
\begin{align}
\mathbb{E}_t f(\xx_{t+1}) &amp;\leq f(\xx_t) - \gamma_t \| \nabla f(\xx_t)\|^2 + \frac{\gamma_t^2 L}{2}\mathbb{E}_t\|\nabla f_i(\xx_t)\|^2\\
\end{align}
Rearranging and assuming that the variance is bounded, i.e., $\mathbb{E}_t\|\nabla f_i(\xx_t)\|^2 < C$ we get
\begin{align}
 \gamma_t \| \nabla f(\xx_t)\|^2 \leq f(\xx_t) - \mathbb{E}_t f(\xx_{t+1}) + \frac{\gamma_t^2 L}{2}\mathbb{E}_t\|\nabla f_i(\xx_t)\|^2
\end{align}
</p> -->

  <dt-appendix class="centered">
  </dt-appendix>

  <script type="text/bibliography">
  @article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  year={1951},
  publisher={JSTOR},
  url={https://projecteuclid.org/euclid.aoms/1177729586}
}
@article{bottou2016optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={arXiv preprint arXiv:1606.04838},
  year={2016}
}

  @article{leblond2018improved,
    title={Improved asynchronous parallel optimization analysis for stochastic incremental methods},
    author={Leblond, R{\'e}mi and Pederegosa, Fabian and Lacoste-Julien, Simon},
    journal={arXiv preprint arXiv:1801.03749},
    url={https://arxiv.org/pdf/1801.03749.pdf},
    year={2018}
  }
  @article{schmidt2014convergence,
  title={Convergence rate of stochastic gradient with constant step size},
  author={Schmidt, Mark},
  url={https://open.library.ubc.ca/cIRcle/collections/facultyresearchandpublications/52383/items/1.0050992},
  year={2014}
}
@article{mandt2017stochastic,
  title={Stochastic Gradient Descent as Approximate Bayesian Inference},
  author={Mandt, Stephan and Hoffman, Matthew D and Blei, David M},
  journal={Journal of Machine Learning Research},
  year={2017}
}
  </script>
<!-- Figure render queue -->
<!-- <script>
// for (var i = 0; i < renderQueue.length; i++) {
//   renderQueue[i](function() {})
//   deleteQueue[i](function() {})
// }
  setTimeout(function() {
    var q = d3.queue(1);

    d3.zip(deleteQueue,renderQueue).forEach(function(fn) {
      q.defer(function(callback) {
        fn[1](callback);
        fn[0](callback);
        renderMath(document.body);
      });
      q.defer(function(callback) {
        setTimeout(function() {
          callback(null);
        }, 50);
      });
    });
    q.await(function(error) {
      if (error) {
        console.error("Render error.", error)
      } else {
        console.log("Render done.")
      }
    });
  }, 50);

// DEBUG
// renderQueue.forEach( function (fn) { fn( function() {}) } )


</script> -->
